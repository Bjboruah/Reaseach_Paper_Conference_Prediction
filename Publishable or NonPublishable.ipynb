{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1 of Feature Engineering and making the the final model**\n"
      ],
      "metadata": {
        "id": "J2b7-VT5C3sr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z9woJspCmNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5819cf30-c5ab-41b4-b89b-177aecc413f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Reference'\n",
        "files = os.listdir(folder_path)\n",
        "print(\"Files in folder:\", files)"
      ],
      "metadata": {
        "id": "FIH5zX5UCve7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146589e7-e3d6-4fb8-ddae-a3e2fd034787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in folder: ['Publishable', 'Non-Publishable', '.DS_Store']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Reference'\n",
        "\n",
        "data = []\n",
        "for label in ['Publishable', 'Non-Publishable']:\n",
        "    label_folder = os.path.join(folder_path, label)\n",
        "    for filename in os.listdir(label_folder):\n",
        "        file_path = os.path.join(label_folder, filename)\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "        except UnicodeDecodeError:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='latin-1') as f:\n",
        "                    text = f.read()\n",
        "            except UnicodeDecodeError:\n",
        "                print(f\"Skipping file {filename} due to encoding issues.\")\n",
        "                continue\n",
        "        data.append({'text': text, 'label': label})\n"
      ],
      "metadata": {
        "id": "KV2DLyUSDKoT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "7565f59a-e804-48fe-a15b-e040ae02e1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: '/content/drive/MyDrive/Reference/Publishable/NeurIPS'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-cbaf6b362873>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/drive/MyDrive/Reference/Publishable/NeurIPS'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "import string\n",
        "import pandas as pd\n",
        "import textstat\n",
        "from textblob import TextBlob\n",
        "import language_tool_python\n",
        "\n",
        "# Initialize LanguageTool\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "# Clean text function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    return text.strip()\n",
        "\n",
        "# Extract sections function\n",
        "def extract_sections(text):\n",
        "    abstract, intro, conclusion = \"\", \"\", \"\"\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    abstract_start = text_lower.find(\"abstract\")\n",
        "    if abstract_start != -1:\n",
        "        abstract_end = text_lower.find(\"introduction\", abstract_start)\n",
        "        abstract = text[abstract_start:abstract_end if abstract_end != -1 else len(text)]\n",
        "\n",
        "    intro_start = text_lower.find(\"introduction\")\n",
        "    if intro_start != -1:\n",
        "        intro_end = text_lower.find(\"conclusion\", intro_start)\n",
        "        intro = text[intro_start:intro_end if intro_end != -1 else len(text)]\n",
        "\n",
        "    conclusion_start = text_lower.find(\"conclusion\")\n",
        "    if conclusion_start != -1:\n",
        "        conclusion = text[conclusion_start:len(text)]\n",
        "\n",
        "    return clean_text(abstract), clean_text(intro), clean_text(conclusion)\n",
        "\n",
        "# Perform sentiment analysis\n",
        "def sentiment_analysis(text):\n",
        "    sentiment = TextBlob(text).sentiment.polarity\n",
        "    return sentiment\n",
        "\n",
        "# Function to check grammar and passive voice\n",
        "def check_writing_quality(text):\n",
        "    matches = tool.check(text)\n",
        "    passive_voice_count = sum(1 for match in matches if 'Passive voice' in match.message)\n",
        "    return {\n",
        "        'grammar_errors': len(matches),\n",
        "        'passive_voice_count': passive_voice_count\n",
        "    }\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Reference'\n",
        "\n",
        "data = []\n",
        "\n",
        "# Process PDF files\n",
        "for category in [\"Publishable\", \"Non-Publishable\"]:\n",
        "    category_path = os.path.join(folder_path, category)\n",
        "\n",
        "    for file_name in os.listdir(category_path):\n",
        "        file_path = os.path.join(category_path, file_name)\n",
        "\n",
        "        if file_name.endswith('.pdf'):\n",
        "            try:\n",
        "                with pdfplumber.open(file_path) as pdf:\n",
        "                    text = \"\"\n",
        "                    for page in pdf.pages:\n",
        "                        page_text = page.extract_text()\n",
        "                        if page_text:\n",
        "                            text += page_text + \"\\n\"\n",
        "\n",
        "                    abstract, intro, conclusion = extract_sections(text)\n",
        "\n",
        "                    readability_score = textstat.flesch_kincaid_grade(conclusion)\n",
        "\n",
        "                    flesch_score = textstat.flesch_reading_ease(conclusion)\n",
        "\n",
        "                    sentiment_score = sentiment_analysis(conclusion)\n",
        "\n",
        "                    writing_quality_features = check_writing_quality(conclusion)\n",
        "\n",
        "                    data.append({\n",
        "                        'paper_name': file_name,\n",
        "                        'abstract': abstract,\n",
        "                        'introduction': intro,\n",
        "                        'conclusion': conclusion,\n",
        "                        'conclusion_length': len(conclusion.split()),\n",
        "                        'readability_score': readability_score,\n",
        "                        'flesch_score': flesch_score,\n",
        "                        'sentiment_score': sentiment_score,\n",
        "                        'grammar_errors': writing_quality_features['grammar_errors'],\n",
        "                        'passive_voice_count': writing_quality_features['passive_voice_count'],\n",
        "                        'label': 0 if category == \"Non-Publishable\" else 1\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df = df.rename(columns={\n",
        "    'paper_name': 'paper_name',\n",
        "    'abstract': 'abstract',\n",
        "    'introduction': 'introduction',\n",
        "    'conclusion': 'conclusion',\n",
        "    'conclusion_length': 'conclusion_length',\n",
        "    'readability_score': 'readability_score',\n",
        "    'flesch_score': 'flesch_score',\n",
        "    'sentiment_score': 'sentiment_score',\n",
        "    'grammar_errors': 'grammar_errors',\n",
        "    'passive_voice_count': 'passive_voice_count',\n",
        "    'label': 'label'\n",
        "})\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(f\"DataFrame created with {len(df)} entries.\")\n",
        "print(df.head(15))\n"
      ],
      "metadata": {
        "id": "9FIqUqtjDNhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(df.columns))\n"
      ],
      "metadata": {
        "id": "Pec1RTemDRa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the required columns\n",
        "selected_columns = ['conclusion_length', 'readability_score', 'flesch_score', 'sentiment_score', 'grammar_errors',\n",
        "                     'label']\n",
        "\n",
        "new_df = df[selected_columns]\n",
        "\n",
        "print(new_df.head())\n"
      ],
      "metadata": {
        "id": "fT95dJnCDWBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X = new_df.drop(columns=['label'])\n",
        "y = new_df['label']\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_features = X.to_numpy()\n",
        "y_labels = y.to_numpy()\n",
        "\n",
        "# Split the data into training and test sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n"
      ],
      "metadata": {
        "id": "lHWzbtqUDXx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lazypredict.Supervised import LazyClassifier\n",
        "import pandas as pd\n",
        "\n",
        "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "\n",
        "# Train and evaluate the models\n",
        "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(models)\n",
        "\n",
        "# Display the top-performing models\n",
        "print(\"\\nTop 5 Models:\")\n",
        "print(models.head(5))\n"
      ],
      "metadata": {
        "id": "O_7TXQyXDaKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(AdaBoostClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.2f}\")\n",
        "\n",
        "best_adaboost = grid_search.best_estimator_\n",
        "y_pred = best_adaboost.predict(X_test)\n",
        "\n",
        "# Re-evaluate the model\n",
        "print(\"Optimized AdaBoostClassifier Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "hyJi1ZSoDcrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Initialize AdaBoostClassifier\n",
        "adaboost_clf = AdaBoostClassifier(n_estimators=50, learning_rate= 0.01 ,random_state=42 )\n",
        "\n",
        "\n",
        "# Train the AdaBoostClassifier on the training data\n",
        "adaboost_clf.fit(X_train, y_train)\n",
        "# Predict on the test data\n",
        "y_pred = adaboost_clf.predict(X_test)\n",
        "# Evaluate the AdaBoostClassifier\n",
        "print(\"AdaBoostClassifier Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "S7nUDnixDhXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "#Joblib for downloading the mode\n",
        "model_path = \"IITKGP.joblib\"\n",
        "joblib.dump(adaboost_clf, model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ],
      "metadata": {
        "id": "f-WDs9QqDjiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2 for using the saved model for doing the prediction:**"
      ],
      "metadata": {
        "id": "NDrErmKAD4GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "model_path = \"/content/IITKGP.joblib\"\n",
        "\n",
        "# Load the saved AdaBoost model\n",
        "loaded_model = joblib.load(model_path)\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "id": "KzIWUwwlEFum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Papers'\n",
        "files = os.listdir(folder_path)\n",
        "print(\"Files in folder:\", files)"
      ],
      "metadata": {
        "id": "Ib8vSXwjEIfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "import string\n",
        "import pandas as pd\n",
        "import textstat\n",
        "from textblob import TextBlob\n",
        "import language_tool_python\n",
        "\n",
        "# Initialize LanguageTool\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "# Clean text function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    return text.strip()\n",
        "\n",
        "# Extract sections function\n",
        "def extract_sections(text):\n",
        "    abstract, intro, conclusion = \"\", \"\", \"\"\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    abstract_start = text_lower.find(\"abstract\")\n",
        "    if abstract_start != -1:\n",
        "        abstract_end = text_lower.find(\"introduction\", abstract_start)\n",
        "        abstract = text[abstract_start:abstract_end if abstract_end != -1 else len(text)]\n",
        "\n",
        "    intro_start = text_lower.find(\"introduction\")\n",
        "    if intro_start != -1:\n",
        "        intro_end = text_lower.find(\"conclusion\", intro_start)\n",
        "        intro = text[intro_start:intro_end if intro_end != -1 else len(text)]\n",
        "\n",
        "    conclusion_start = text_lower.find(\"conclusion\")\n",
        "    if conclusion_start != -1:\n",
        "        conclusion = text[conclusion_start:len(text)]\n",
        "\n",
        "    return clean_text(abstract), clean_text(intro), clean_text(conclusion)\n",
        "\n",
        "# Perform sentiment analysis\n",
        "def sentiment_analysis(text):\n",
        "    sentiment = TextBlob(text).sentiment.polarity\n",
        "    return sentiment\n",
        "\n",
        "# Function to check grammar and passive voice\n",
        "def check_writing_quality(text):\n",
        "    matches = tool.check(text)\n",
        "    passive_voice_count = sum(1 for match in matches if 'Passive voice' in match.message)\n",
        "    return {\n",
        "        'grammar_errors': len(matches),\n",
        "        'passive_voice_count': passive_voice_count\n",
        "    }\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Papers'\n",
        "\n",
        "data = []\n",
        "\n",
        "# Process PDF files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    if file_name.endswith('.pdf'):\n",
        "        try:\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                text = \"\"\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "\n",
        "                abstract, intro, conclusion = extract_sections(text)\n",
        "\n",
        "                readability_score = textstat.flesch_kincaid_grade(conclusion)\n",
        "\n",
        "                flesch_score = textstat.flesch_reading_ease(conclusion)\n",
        "\n",
        "                sentiment_score = sentiment_analysis(conclusion)\n",
        "\n",
        "                writing_quality_features = check_writing_quality(conclusion)\n",
        "\n",
        "                data.append({\n",
        "                    'paper_name': file_name,\n",
        "                    'abstract': abstract,\n",
        "                    'introduction': intro,\n",
        "                    'conclusion': conclusion,\n",
        "                    'conclusion_length': len(conclusion.split()),\n",
        "                    'readability_score': readability_score,\n",
        "                    'flesch_score': flesch_score,\n",
        "                    'sentiment_score': sentiment_score,\n",
        "                    'grammar_errors': writing_quality_features['grammar_errors'],\n",
        "                    'passive_voice_count': writing_quality_features['passive_voice_count'],\n",
        "                    'label': 0\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df = df.rename(columns={\n",
        "    'paper_name': 'paper_name',\n",
        "    'abstract': 'abstract',\n",
        "    'introduction': 'introduction',\n",
        "    'conclusion': 'conclusion',\n",
        "    'conclusion_length': 'conclusion_length',\n",
        "    'readability_score': 'readability_score',\n",
        "    'flesch_score': 'flesch_score',\n",
        "    'sentiment_score': 'sentiment_score',\n",
        "    'grammar_errors': 'grammar_errors',\n",
        "    'passive_voice_count': 'passive_voice_count',\n",
        "    'label': 'label'\n",
        "})\n",
        "\n",
        "print(f\"DataFrame created with {len(df)} entries.\")\n",
        "print(df.head(15))"
      ],
      "metadata": {
        "id": "bSGQJV4_ELX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(df.columns))\n"
      ],
      "metadata": {
        "id": "coTMl6tbENcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns = ['conclusion_length', 'readability_score', 'flesch_score', 'sentiment_score', 'grammar_errors']\n",
        "\n",
        "# Create a new DataFrame with only these columns\n",
        "new_df = df[selected_columns]\n",
        "\n",
        "print(new_df.head())\n"
      ],
      "metadata": {
        "id": "eJLVB6hiERdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the new_df\n",
        "predictions = loaded_model.predict(new_df)\n",
        "\n",
        "\n",
        "new_df['predicted_label'] = predictions\n",
        "\n",
        "print(new_df.head())"
      ],
      "metadata": {
        "id": "g3EJXIbeEUCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.concat([df['paper_name'], new_df['predicted_label']], axis=1)\n",
        "\n",
        "final_df['predicted_label'] = final_df['predicted_label'].replace({0: 'Unpublishable', 1: 'Publishable'})\n",
        "\n",
        "# Sort the DataFrame based on 'paper_name' without altering the serial number\n",
        "final_df = final_df.sort_values(by='paper_name', ascending=True)\n",
        "\n",
        "# Save the sorted DataFrame to a CSV file, without writing the index\n",
        "output_path = '/content/final_predictions.csv'\n",
        "final_df.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "PV8ClC6sEV8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-7q5qtSjEXzG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}